{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1xahQm7Qp9Aqrg65suQm1aaYGQQ1yQI9F",
      "authorship_tag": "ABX9TyPTs7ocGpn2d87RSUZLh+6v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changhorang/SSAC_study/blob/main/DL3%20(PyTorch)/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC/Chapter2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2-3_어휘_집합_구축하기"
      ],
      "metadata": {
        "id": "wxu6PZ407nUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BPE 기반 토크나이저 만들기\n",
        "### 공통 실습환경\n",
        "- install ratsnlp"
      ],
      "metadata": {
        "id": "eIaHi89I3WJv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_F2qoYM22cn",
        "outputId": "94b04760-531f-46f9-9b2a-4e4850a400d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ratsnlp\n",
            "  Downloading ratsnlp-0.0.9999-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.2 MB/s \n",
            "\u001b[?25hCollecting transformers==4.10.0\n",
            "  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 9.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: flask>=1.1.4 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.1.4)\n",
            "Collecting Korpora>=0.2.0\n",
            "  Downloading Korpora-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 3.5 MB/s \n",
            "\u001b[?25hCollecting flask-ngrok>=0.0.25\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.10.0+cu111)\n",
            "Collecting pytorch-lightning==1.3.4\n",
            "  Downloading pytorch_lightning-1.3.4-py3-none-any.whl (806 kB)\n",
            "\u001b[K     |████████████████████████████████| 806 kB 25.9 MB/s \n",
            "\u001b[?25hCollecting flask-cors>=3.0.10\n",
            "  Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n",
            "Collecting torchmetrics>=0.2.0\n",
            "  Downloading torchmetrics-0.6.2-py3-none-any.whl (332 kB)\n",
            "\u001b[K     |████████████████████████████████| 332 kB 67.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (4.62.3)\n",
            "Requirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (2.7.0)\n",
            "Collecting PyYAML<=5.4.1,>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 51.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (1.19.5)\n",
            "Collecting fsspec[http]>=2021.4.0\n",
            "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 46.4 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate==0.3.0\n",
            "  Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (21.3)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 29.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0->ratsnlp) (4.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0->ratsnlp) (3.4.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 22.4 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 33.7 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 247 kB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0->ratsnlp) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0->ratsnlp) (2.23.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.1.4->ratsnlp) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask>=1.1.4->ratsnlp) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask>=1.1.4->ratsnlp) (1.1.0)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.1.4->ratsnlp) (7.1.2)\n",
            "Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors>=3.0.10->ratsnlp) (1.15.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 44.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers==4.10.0->ratsnlp) (3.10.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask>=1.1.4->ratsnlp) (2.0.1)\n",
            "Collecting dataclasses>=0.6\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Collecting xlrd>=1.2.0\n",
            "  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pytorch-lightning==1.3.4->ratsnlp) (3.0.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0->ratsnlp) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0->ratsnlp) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0->ratsnlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0->ratsnlp) (3.0.4)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (1.42.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (1.8.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (57.4.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (0.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (0.37.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (0.12.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.10.0->ratsnlp) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (3.1.1)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 23.7 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.4->ratsnlp) (2.0.8)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.4->ratsnlp) (21.2.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 47.3 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 41.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0->ratsnlp) (1.1.0)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=9c17452993359b704640aaa7b2ad968dcd7a52fce77a7343a31c224898fef443\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built future\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, PyYAML, fsspec, aiohttp, xlrd, torchmetrics, tokenizers, sacremoses, pyDeprecate, huggingface-hub, future, dataclasses, transformers, pytorch-lightning, Korpora, flask-ngrok, flask-cors, ratsnlp\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 1.1.0\n",
            "    Uninstalling xlrd-1.1.0:\n",
            "      Successfully uninstalled xlrd-1.1.0\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed Korpora-0.2.0 PyYAML-5.4.1 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 dataclasses-0.6 flask-cors-3.0.10 flask-ngrok-0.0.25 frozenlist-1.2.0 fsspec-2021.11.1 future-0.18.2 huggingface-hub-0.2.1 multidict-5.2.0 pyDeprecate-0.3.0 pytorch-lightning-1.3.4 ratsnlp-0.0.9999 sacremoses-0.0.46 tokenizers-0.10.3 torchmetrics-0.6.2 transformers-4.10.0 xlrd-2.0.1 yarl-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install ratsnlp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 말뭉치 내려받기 및 전처리\n",
        "- 네이버 영화리뷰 NSMC"
      ],
      "metadata": {
        "id": "GGF626s53nvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Korpora import Korpora\n",
        "nsmc = Korpora.load('nsmc', force_download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ynv3pNU3Fxf",
        "outputId": "c66af724-c4f0-40d0-e49c-eb08ef2793c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : e9t@github\n",
            "    Repository : https://github.com/e9t/nsmc\n",
            "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
            "\n",
            "    Naver sentiment movie corpus v1.0\n",
            "    This is a movie review dataset in the Korean language.\n",
            "    Reviews were scraped from Naver Movies.\n",
            "\n",
            "    The dataset construction is based on the method noted in\n",
            "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
            "\n",
            "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nsmc] download ratings_train.txt: 14.6MB [00:00, 76.8MB/s]                            \n",
            "[nsmc] download ratings_test.txt: 4.90MB [00:00, 38.4MB/s]                            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def write_lines(path, lines):\n",
        "  with open(path, 'w', encoding='utf-8') as f:\n",
        "    for line in lines:\n",
        "      f.write(f'{line}\\n')\n",
        "\n",
        "write_lines('/root/train.txt', nsmc.train.get_all_texts())\n",
        "write_lines('/root/test.txt', nsmc.test.get_all_texts())"
      ],
      "metadata": {
        "id": "c-KYGul130dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT 토크나이저 구축\n",
        "- GPT 계열 모델 토크나이저 기법 : BPE"
      ],
      "metadata": {
        "id": "uk5jMRB94lrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs('/content/drive/MyDrive/nlpbook/bbpe', exist_ok=True)"
      ],
      "metadata": {
        "id": "4H5wbmOz4gsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "bytebpe_tokenizer = ByteLevelBPETokenizer()\n",
        "bytebpe_tokenizer.train(\n",
        "    files=['/root/train.txt', '/root/test.txt'], # 학습할 말뭉치를 리스트 형태로 넣기\n",
        "    vocab_size=10000, # 어휘 집합 크기 조절\n",
        "    special_tokens=['[PAD]'] # 특수 토큰 추가\n",
        ")\n",
        "bytebpe_tokenizer.save_model('/content/drive/MyDrive/nlpbook/bbpe')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_Eivo3O5G-e",
        "outputId": "016c910d-736f-4847-d945-772f981e80d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/nlpbook/bbpe/vocab.json',\n",
              " '/content/drive/MyDrive/nlpbook/bbpe/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT 토크나이저 구축"
      ],
      "metadata": {
        "id": "lLTYFWc-6vr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs('/content/drive/MyDrive/nlpbook/wordpiece', exist_ok=True)"
      ],
      "metadata": {
        "id": "sizitiRx6Z0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "wordpiece_tokenizer = BertWordPieceTokenizer(lowercase=False)\n",
        "wordpiece_tokenizer.train(\n",
        "    files=['/root/train.txt', '/root/test.txt'], # 학습할 말뭉치를 리스트 형태로 넣기\n",
        "    vocab_size=10000 # 어휘 집합 크기 조절\n",
        ")\n",
        "wordpiece_tokenizer.save_model('/content/drive/MyDrive/nlpbook/wordpiece')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFqiL4mi7MB3",
        "outputId": "e295630d-497b-4da8-f932-e485cf3a51c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/nlpbook/wordpiece/vocab.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-4 토큰화하기\n",
        "### GPT 입력값 만들기"
      ],
      "metadata": {
        "id": "UrJRIzip74OP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tokenizers\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer_gpt = GPT2Tokenizer.from_pretrained('/content/drive/MyDrive/nlpbook/bbpe')\n",
        "tokenizer_gpt.pad_token = '[PAD]'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FD1AzgQ5FIC",
        "outputId": "01fa7f7d-7fd2-481d-fec5-619e6ecda00d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "file /content/drive/MyDrive/nlpbook/bbpe/config.json not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "  \"아 더빙.. 진짜 짜증나네요 목소리\",\n",
        "  \"흠...포스터보고 초딩영환줄...오버연기조차 가볍지 않구나\",\n",
        "  \"별루 였다..\"\n",
        "]\n",
        "\n",
        "tokenized_sentences = [tokenizer_gpt.tokenize(sentence) for sentence in sentences]\n",
        "tokenized_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaV3kloQ-cke",
        "outputId": "6998a76e-6ce5-4052-b492-360017bc59ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['ìķĦ', 'ĠëįĶë¹Ļ', '..', 'Ġì§Ħì§ľ', 'Ġì§ľì¦ĿëĤĺ', 'ëĦ¤ìļĶ', 'Ġëª©ìĨĮë¦¬'],\n",
              " ['íĿł',\n",
              "  '...',\n",
              "  'íı¬ìĬ¤íĦ°',\n",
              "  'ë³´ê³ł',\n",
              "  'Ġì´ĪëĶ©',\n",
              "  'ìĺģíĻĺ',\n",
              "  'ì¤Ħ',\n",
              "  '...',\n",
              "  'ìĺ¤ë²Ħ',\n",
              "  'ìĹ°ê¸°',\n",
              "  'ì¡°ì°¨',\n",
              "  'Ġê°Ģë³į',\n",
              "  'ì§Ģ',\n",
              "  'ĠìķĬ',\n",
              "  'êµ¬ëĤĺ'],\n",
              " ['ë³Ħë£¨', 'Ġìĺ', 'Ģëĭ¤', '..']]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 실제 모델 입력 값\n",
        "batch_inputs = tokenizer_gpt(\n",
        "    sentences,\n",
        "    padding='max_length', # 문자의 최대 길이에 맞춰 패딩\n",
        "    max_length=12, # 문장의 토큰 기준 최대 길이\n",
        "    truncation=True # 문장 잘림 허용옵션\n",
        ")\n",
        "batch_inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeATJWrs--2k",
        "outputId": "dcb8cd28-2224-44c3-e0c2-52dac8c8dbf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[334, 2338, 263, 581, 4055, 464, 3808, 0, 0, 0, 0, 0], [3693, 336, 2876, 758, 2883, 5048, 806, 336, 9875, 875, 2960, 7292], [4957, 451, 3653, 263, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input_ids: 토큰화 결과를 토대로 index화 (indexing)\n",
        "batch_inputs['input_ids']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8v5dQuT_i72",
        "outputId": "c06d681d-7a5b-472a-f33e-26002558e5a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[334, 2338, 263, 581, 4055, 464, 3808, 0, 0, 0, 0, 0],\n",
              " [3693, 336, 2876, 758, 2883, 5048, 806, 336, 9875, 875, 2960, 7292],\n",
              " [4957, 451, 3653, 263, 0, 0, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# attention_mask: 단어 인 곳 = 1 / padding부분 = 0\n",
        "batch_inputs['attention_mask']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUrpanc9AVGE",
        "outputId": "4e50f96b-3a77-46a3-dba0-53ec5ffce0d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
              " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT 입력값 만들기"
      ],
      "metadata": {
        "id": "XMdDAHnoAmRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer_bert = BertTokenizer.from_pretrained('/content/drive/MyDrive/nlpbook/wordpiece', do_lower_case=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqlRArA_AlXG",
        "outputId": "918b2b50-e747-4e1a-aa5a-00a39f93665d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "file /content/drive/MyDrive/nlpbook/wordpiece/config.json not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "  \"아 더빙.. 진짜 짜증나네요 목소리\",\n",
        "  \"흠...포스터보고 초딩영환줄...오버연기조차 가볍지 않구나\",\n",
        "  \"별루 였다..\"\n",
        "]\n",
        "\n",
        "tokenized_sentences = [tokenizer_bert.tokenize(sentence) for sentence in sentences]\n",
        "tokenized_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnGT3SA2BFG-",
        "outputId": "efafb6da-7539-46d6-9a32-e750acdf64d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['아', '더빙', '.', '.', '진짜', '짜증나', '##네요', '목소리'],\n",
              " ['흠',\n",
              "  '.',\n",
              "  '.',\n",
              "  '.',\n",
              "  '포스터',\n",
              "  '##보고',\n",
              "  '초딩',\n",
              "  '##영환',\n",
              "  '##줄',\n",
              "  '.',\n",
              "  '.',\n",
              "  '.',\n",
              "  '오버',\n",
              "  '##연기',\n",
              "  '##조차',\n",
              "  '가볍',\n",
              "  '##지',\n",
              "  '않',\n",
              "  '##구나'],\n",
              " ['별루', '였다', '.', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 실제 입력\n",
        "batch_inputs = tokenizer_bert(\n",
        "    sentences,\n",
        "    padding='max_length',\n",
        "    max_length=12,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "batch_inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBgSYD6RBgf2",
        "outputId": "62a07312-8724-4db1-a804-caf7d128d0cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[2, 621, 2631, 16, 16, 1993, 3678, 1990, 3323, 3, 0, 0], [2, 997, 16, 16, 16, 2609, 2045, 2796, 4917, 1263, 16, 3], [2, 3274, 9507, 16, 16, 3, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_inputs['input_ids']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUaCTpJdC3yW",
        "outputId": "787d5006-ee1e-4048-f13c-8d2f5f2259db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 621, 2631, 16, 16, 1993, 3678, 1990, 3323, 3, 0, 0],\n",
              " [2, 997, 16, 16, 16, 2609, 2045, 2796, 4917, 1263, 16, 3],\n",
              " [2, 3274, 9507, 16, 16, 3, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_inputs['attention_mask']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbL68cIgDD6x",
        "outputId": "df5be79d-f0ea-4931-aac5-a3bdd204df9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
              " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# token_type_ids: BERT모델은 문서 혹은 문장을 입력 받음 / 입력받는 segment의 정보를 의미\n",
        "batch_inputs['token_type_ids']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAxlQMfODX_p",
        "outputId": "74f01ee4-cb71-4397-88b8-ad00f60e98b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    }
  ]
}