{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch.8 영상 매칭과 추적\n",
    "## 8.1 비슷한 그림 찾기\n",
    "### 8.1.1 평균 해시 매칭\n",
    "- 픽셀 전체의 평균 값을 구해 평균보다 크면 1, 작으면 0으로 변환\n",
    "\n",
    "### 유클리드 거리 & 해밍 거리\n",
    "- 평균 해시 매칭을 통해 구한 영상들로 비슷한 정도를 측정하기 위한 방법 <br>\n",
    "*** 유클리드 거리 : 유클리드 거리는 두값의 차리로 거리를 계산 ***\n",
    "*** 해밍 거리 : 해밍 거리는 두값의 길이가 같을 때, 같은 자리에서의 값이 서로 다른 것이 몇 개인지를 비교 *** <br>\n",
    "- 영상에서는 해밍 거리가 더 적합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/ChangHo Kim/Downloads/folder/101_ObjectCategories\\Faces_easy\\image_0001.jpg 0.0\n"
     ]
    }
   ],
   "source": [
    "# tar.gz 파일 압축해제\n",
    "# cmd에서 파일의 directory로 이동 -> tar -zxvf [압축파일명] -C [new folder]\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# 영상 읽기 및 표시\n",
    "img = cv2.imread('img/image_0001.jpg')\n",
    "cv2.imshow('query', img)\n",
    "\n",
    "# 비교할 영상들이 있는 경로 \n",
    "search_dir = 'C:/Users/ChangHo Kim/Downloads/folder/101_ObjectCategories'\n",
    "\n",
    "# 이미지를 16x16 크기의 평균 해쉬로 변환 \n",
    "def img2hash(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.resize(gray, (16, 16))\n",
    "    avg = gray.mean()\n",
    "    bi = 1 * (gray > avg)\n",
    "    return bi\n",
    "\n",
    "# 해밍거리 측정 함수\n",
    "def hamming_distance(a, b):\n",
    "    a = a.reshape(1,-1)\n",
    "    b = b.reshape(1,-1)\n",
    "    # 같은 자리의 값이 서로 다른 것들의 합\n",
    "    distance = (a != b).sum()\n",
    "    return distance\n",
    "\n",
    "# 권총 영상의 해쉬 구하기\n",
    "query_hash = img2hash(img)\n",
    "\n",
    "# 이미지 데이타 셋 디렉토리의 모든 영상 파일 경로\n",
    "img_path = glob.glob(search_dir+'/**/*.jpg')\n",
    "for path in img_path:\n",
    "    # 데이타 셋 영상 한개 읽어서 표시\n",
    "    img = cv2.imread(path)\n",
    "    cv2.imshow('searching...', img)\n",
    "    cv2.waitKey(5)\n",
    "    # 데이타 셋 영상 한개의 해시 \n",
    "    a_hash = img2hash(img)\n",
    "    # 해밍 거리 산출 ---⑧\n",
    "    dst = hamming_distance(query_hash, a_hash)\n",
    "    if dst/256 < 0.10: # 해밍거리 10% 이내만 출력\n",
    "        print(path, dst/256)\n",
    "        cv2.imshow(path, img)\n",
    "cv2.destroyWindow('searching...')\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.2 템플릿 매칭\n",
    "- result = cv2.matchTemplate(img, template, method[, result, mask])\n",
    "    - template : 템플릿 영상\n",
    "    - method : 매칭 메서드\n",
    "        - cv2.TM_SQDIFF : 제곱 차이 매칭, 완벽 매칭: 0, 나쁜 매칭: 큰 값\n",
    "        - cv2.TM_SQDIFF_NORMED : 제곱 차이 매칭의 정규화\n",
    "        - cv2.TM_CCORR : 상관관계 매칭, 완벽 매칭: 큰 값, 나쁜 매칭: 0\n",
    "        - cv2.TM_CCORR_NORMED : 상관관계 매칭의 정규화\n",
    "        - cv2.TM_CCOEFF : 상관계수 매칭, 완벽 매칭: 1, 나쁜 매칭: -1\n",
    "        - cv2.TM_CCOEFF_NORMED : 상관계수 매칭의 정규화\n",
    "    - result : 매칭 결과, (W-w+1)x(H-h+1) 크기의 2차원 배열\n",
    "        - W, H : img의 열과 행\n",
    "        - w, h : template의 열과 행\n",
    "    - mask : TM_SQDIFF, TM_CCORR_NORMED인 경우 사용할 마스크\n",
    "\n",
    "- minVal, maxVal, minLoc, maxLoc = cv2.minMaxLoc(src[, mask])\n",
    "    - src : 입력 1채널 배열\n",
    "    - minVal, maxVal : 배열 전체에서 최소, 최대 값\n",
    "    - minLoc, maxLoc : 최소 값과 최대 값의 좌표 (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv2.TM_CCOEFF_NORMED -0.1780252307653427 0.5131933093070984 (42, 0) (208, 43)\n",
      "cv2.TM_CCORR_NORMED 0.827332615852356 0.9238022565841675 (85, 6) (208, 43)\n",
      "cv2.TM_SQDIFF_NORMED 0.17028295993804932 0.36860838532447815 (208, 43) (86, 7)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('img/figures.jpg')\n",
    "template = cv2.imread('img/taekwonv1.jpg')\n",
    "th, tw = template.shape[:2]\n",
    "cv2.imshow('template', template)\n",
    "\n",
    "# 세가지 매칭 메서드 순회\n",
    "methods = ['cv2.TM_CCOEFF_NORMED', 'cv2.TM_CCORR_NORMED', 'cv2.TM_SQDIFF_NORMED']\n",
    "\n",
    "for i, method_name in enumerate(methods):\n",
    "    img_draw = img.copy()\n",
    "    method = eval(method_name)\n",
    "    # 템플릿 매칭\n",
    "    res = cv2.matchTemplate(img, template, method)\n",
    "    # 최대, 최소 값과 그 좌표 구하기\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n",
    "    print(method_name, min_val, max_val, min_loc, max_loc)\n",
    "\n",
    "    # TM_SQDIFF_NORMED의 경우 최소 값이 좋은 매칭, 나머지는 그 반대\n",
    "    if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:\n",
    "        top_left = min_loc\n",
    "        match_val = min_val\n",
    "\n",
    "    else:\n",
    "        top_left = max_loc\n",
    "        match_val = max_val\n",
    "    \n",
    "    # 매칭 좌표를 구해서 사각형 표시\n",
    "    bottom_right = (top_left[0] + tw, top_left[1] + th)\n",
    "    cv2.rectangle(img_draw, top_left, bottom_right, (0, 0, 255), 2)\n",
    "\n",
    "    # 매칭 포인트 표시\n",
    "    cv2.putText(img_draw, str(match_val), top_left, cv2.FONT_HERSHEY_PLAIN, 2, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "    cv2.imshow(method_name, img_draw)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 영상의 특징과 키 포인트\n",
    "### 8.2.1 코너 특징 검출\n",
    "- corner : 엣지와 엣지가 만나는 곳\n",
    "- dst = cv2.cornerHarris(src, blockSize, ksize, k[, dst, borderType])\n",
    "    - src : 그레이 스케일\n",
    "    - blockSize : 이웃 픽셀 범위\n",
    "    - ksize : 코너 검출 상수 (경험적 상수: 0.04~0.06)\n",
    "    - dst : 검출 결과 (src와 같은 크기의 1채널 배열, 변화량의 값, 지역 최대 값이 코너점을 의미)\n",
    "    - borderType : 외곽 영역 보정 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('img/house.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# 해리스 코너 검출\n",
    "corner = cv2.cornerHarris(gray, 2, 3, 0.04)\n",
    "# 변화량 결과의 최대값 10% 이상의 좌표 구하기\n",
    "coord = np.where(corner > 0.1*corner.max())\n",
    "coord = np.stack((coord[1], coord[0]), axis=-1)\n",
    "\n",
    "# 코너 좌표에 동그라미 그리기\n",
    "for x, y in coord:\n",
    "    cv2.circle(img, (x, y), 5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "# 변화량을 영상으로 표현하기 위해서 0~255로 정규화\n",
    "corner_norm = cv2.normalize(corner, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "# 화면에 출력\n",
    "corner_norm = cv2.cvtColor(corner_norm, cv2.COLOR_GRAY2BGR)\n",
    "merged = np.hstack((corner_norm, img))\n",
    "cv2.imshow('Harris Corner', merged)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시와 토마시가 개선한 알고리즘\n",
    "- corners = cv2.goodFeaturesToTrack(img, maxCorners, qualityLevel, minDistance[, corners, mask, blockSize, useHarrisDetector, k])\n",
    "    - maxCorners : 얻고 싶은 코너 개수, 강한 것 순\n",
    "    - qualityLevel : 코너로 판단할 스레시홀드 값\n",
    "    - minDistance : 코너 간 최소 거리\n",
    "    - blockSize = 3 : 코너 주변 영역의 크기\n",
    "    - useHarrisDetector=False : 코너 검출 방법 선택\n",
    "        - True : 해리스 코너 검출 방법 / False : 시와 토마시 검출 방법\n",
    "    - k : 해리스 코너 검출 방법에 사용할 k 개수\n",
    "    - corners : 코너 검출 좌표 결과, (Nx1x2) 크기의 배열, 실수 값이므로 정수로 변형 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('img/house.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# 시-토마스 코너 검출\n",
    "corners = cv2.goodFeaturesToTrack(gray, 80, 0.01, 20)\n",
    "# 실수 좌표를 점수 좌표로 변환\n",
    "corners = np.int32(corners)\n",
    "\n",
    "# 코너 좌표에 동그라미 그리기\n",
    "for corner in corners:\n",
    "    x, y = corner[0]\n",
    "    cv2.circle(img, (x, y), 5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "cv2.imshow('Corners', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2 키 포인트와 특징 검출기\n",
    "- keypoints = detector.detect(img, [, mask]) : 키포인트 검출 함수\n",
    "    - img : 입력 (바이너리 스케일)\n",
    "    - mask : 검출 제외 마스크\n",
    "    - keypoints : 특징점 검출 결과 (list)\n",
    "<br>\n",
    "- KeyPoint : 특징점 정보를 담는 객체\n",
    "    - pt : 키포인트(x, y) 좌표, float 타입으로 정수로 변환 필요\n",
    "    - size : 의미 있는 키 포인트 이웃의 반지름\n",
    "    - angle : 특징점 방향 (시계 방향, -1=의미 없음)\n",
    "    - response : 특징점 반응 강도\n",
    "    - octave : 발견된 이미지 피라미드 계층\n",
    "    - class_id : 키포인트가 속한 객체 ID\n",
    "<br>\n",
    "- 키포인트를 영상에 표시해주는 전용 함수\n",
    "- outImg = cv2.drawKeypoints(img, keypoints, outImg[, color[, flags]])\n",
    "    - keypoints : 표시할 키 포인트 리스트\n",
    "    - outImg : result\n",
    "    - color : 표시할 색상 (default: random)\n",
    "    - flags : 표시 방법 선택 플래그\n",
    "        - cv2.DRAW_MATCHES_FLAGS_DEFAULT : 좌표 중심에 동그라미만 그림 (default)\n",
    "        - cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS : 동그라미의 크리를 size와 angle 반영해서 그림\n",
    "<br>\n",
    "<br>\n",
    "### 8.2.3 GFTTDetector\n",
    "- detector = cv2.GFTTDetector_create([, maxCorners[, qualityLevel, minDistance, blockSize, useHarrisDetector, k]])\n",
    "    - 인자의 모든 내용은 cv2.goodFeaturesToTrack()과 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('img/house.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# gftt (good feature to trac)\n",
    "gftt = cv2.GFTTDetector_create()\n",
    "# key pts\n",
    "keypoints = gftt.detect(gray, None)\n",
    "# key pts 그리기\n",
    "img_draw = cv2.drawKeypoints(img, keypoints, None)\n",
    "\n",
    "cv2.imshow('GFTT detector', img_draw)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.4 FAST (feature from accelerated segment test)\n",
    "- detector = cv2.FastFeatureDetector_create([threshold[, nomaxSuppresion, type]])\n",
    "    - threshold = 10 : 코너 판단 임계값\n",
    "    - nomaxSuppresion = True : 최대 점수가 아닌 코너 억제\n",
    "    - type : edge 검출 패턴\n",
    "        - cv2.FastFeatureDetector_TYPE_9_16 : 16개 중 9개 연속 (기본값)\n",
    "        - cv2.FastFeatureDetector_TYPE_7_12 : 12개 중 7개 연속\n",
    "        - cv2.FastFeatureDetector_TYPE_5_8 : 8개 중 5개 연속"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('img/house.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# gftt (good feature to trac)\n",
    "fast = cv2.FastFeatureDetector_create(100)\n",
    "# key pts\n",
    "keypoints = fast.detect(gray, None)\n",
    "# key pts 그리기\n",
    "img = cv2.drawKeypoints(img, keypoints, None)\n",
    "\n",
    "cv2.imshow('GFTT detector', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.5 SimpleBlobDetector\n",
    "- Binaray Large Object (BLOB)는 바이너리 스케일 이미지의 연결된 픽셀 그룹을 의미함\n",
    "- 자잘한 객체는 노이즈로 판단하고 특정 크기 이상의 큰 객체에만 관심을 두는 방법\n",
    "- detector = cv2.SimpleBlobDetector_create([parameters]) : BLOB 검출기 생성자\n",
    "- cv2.SimpleBlobDetector_Params()\n",
    "    - minThreshold, maxThreshold, thresholdStep : BLOB를 생성하기 위한 경계 값 (minThreshold에서 maxThreshold를 넘지않을 때 까지 thresholdStep만큼 증가)\n",
    "    - minRepeatability : BLOB에 참여하기 위한 연속된 경계 값의 개수\n",
    "    - minDistBetweenBlobs : 두 BLOB를 하나의 BLOB로 간주한 거리\n",
    "    - filterByArea : 면적 필터 옵션\n",
    "    - minArea, maxArea : min~max범위의 면적만 BLOB로 검출\n",
    "    - filterByCircularity : 원형 비율 필터 옵션\n",
    "    - minCircularity, maxCircularity : min~max범위의 원형의 비율만 BLOB로 검출\n",
    "    - filterByColor : 밝기를 이용한 필터 옵션\n",
    "    - blobColor : 0 = black / 255 = white BLOB검출\n",
    "    - filterByConvexity : 볼록 비율 필터 옵션\n",
    "    - minConvexity, maxConvexity : min~max범위의 볼록 비율만 BLOB로 검출\n",
    "    - filterByInertia : 관성 비율 필터 옵션\n",
    "    - minInertiaRatio, maxInertiaRatio : min~max범위의 관성 비율만 BLOB로 검출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('img/house.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# SimpleBlobDetector_create\n",
    "detector = cv2.SimpleBlobDetector_create()\n",
    "# key pts\n",
    "keypoints = detector.detect(gray)\n",
    "# key pts 그리기\n",
    "img = cv2.drawKeypoints(img, keypoints, None, (0, 0, 255), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "cv2.imshow('BLOB', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('img/house.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# SimpleBlobDetector_parameter 생성\n",
    "params = cv2.SimpleBlobDetector_Params()\n",
    "\n",
    "# 경계값 조정\n",
    "params.minThreshold = 10\n",
    "params.maxThreshold = 240\n",
    "params.thresholdStep = 5\n",
    "\n",
    "# 면적 필터를 켜고 최소 값 지정\n",
    "params.filterByArea = True\n",
    "params.minArea = 200\n",
    "\n",
    "# 컬러, 볼록 비율, 원형 비율 필터 옵션 끄기\n",
    "params.filterByColor = False\n",
    "params.filterByConvexity = False\n",
    "params.filterByInertia = False\n",
    "params.filterByCircularity = False\n",
    "\n",
    "# SimpleBlobDetector_create\n",
    "detector = cv2.SimpleBlobDetector_create(params)\n",
    "# key pts\n",
    "keypoints = detector.detect(gray)\n",
    "# key pts 그리기\n",
    "img = cv2.drawKeypoints(img, keypoints, None, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "cv2.imshow('BLOB with Params', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 디스크립터 추출기\n",
    "### 8.3.1 특징 디스크립터와 추출기\n",
    "- 특징 디스크립터를 추출하기 위한 함수\n",
    "- keypoints, descriptors = detector.compute(img, keypoints[, descriptors]) : 키포인트를 전달하면 특징 디스크립터를 계산해서 반환\n",
    "- keypoints, descriptors = detector.detectAndCompute(img, mask[, descriptors, useProvideKeypoints]) : 키포인트 검출과 특징 디스크립터 계산을 한 번에 수행\n",
    "    - keypoints : 디스크립터 계산을 위해 사용할 키포인트\n",
    "    - descriptors : 계산된 디스크립터\n",
    "    - mask : 키포인트 검출에 사용할 마스크\n",
    "    - useProvideKeypoints : True 인 경우 키포인트 검출을 수행하지 않음 (사용 x)\n",
    "\n",
    "### 8.3.2 SIFT (scale-invariant feature transform)\n",
    "- 이미지 피라미드를 이용해 크기 변화에 따른 특징 검출의 문제를 해결한 알고리즘\n",
    "- detector = cv2.xfeatures2d.SIFT_create([, nfeatures[, nOctavelLayers[, contrastThreshold[, edgeThreshold[, sigma]]]]])\n",
    "    - nfeatures : 검출 최대 특징 수\n",
    "    - nOctavelLayers : 이미지 피라미드에 사용할 계층 수\n",
    "    - contrastThreshold : 필터링할 빈약한 특징 문턱 값\n",
    "    - edgeThreshold : 필터링할 엣지 문턱 값\n",
    "    - sigma : 이미지 피라미드 0계층에서 사용할 가우시안 필터의 시그마 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints:  413 descriptor:  (413, 128)\n",
      "[[  1.   1.   1. ...   0.   0.   1.]\n",
      " [  8.  24.   0. ...   1.   0.   4.]\n",
      " [  0.   0.   0. ...   0.   0.   2.]\n",
      " ...\n",
      " [  1.   8.  71. ...  73. 127.   3.]\n",
      " [ 35.   2.   7. ...   0.   0.   9.]\n",
      " [ 36.  34.   3. ...   0.   0.   1.]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('img/house.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# SIFT 생성\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "# key pts\n",
    "keypoints, descriptor = sift.detectAndCompute(gray, None)\n",
    "print('keypoints: ', len(keypoints), 'descriptor: ', descriptor.shape)\n",
    "print(descriptor)\n",
    "\n",
    "# key pts 그리기\n",
    "img = cv2.drawKeypoints(img, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "cv2.imshow('SIFT', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 특징 매칭 \n",
    "- 서로 다른 두 영상에서 구한 키포인트와 특징 디스크립터들을 각각 비교해 그 거리가 비슷한 것끼리 짝짓는 것을 의미\n",
    "\n",
    "### 8.4.1 특징 매칭 인터페이스\n",
    "\n",
    "### 8.4.2 BFMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img1 = cv2.imread('img/taekwonv1.jpg')\n",
    "img2 = cv2.imread('img/figures.jpg')\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# sift 디스크립터 추출기 생성\n",
    "detector = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "# 각 영상에 대해 키포인트와 디스크립터 추출\n",
    "kp1, desc1 = detector.detectAndCompute(gray1, None)\n",
    "kp2, desc2 = detector.detectAndCompute(gray2, None)\n",
    "\n",
    "# BFMatcher 생성, L1거리, 상호체크\n",
    "matcher = cv2.BFMatcher(cv2.NORM_L1, crossCheck=True)\n",
    "# 매칭 계산\n",
    "matches = matcher.match(desc1, desc2)\n",
    "# 매칭 결과 그리기\n",
    "res = cv2.drawMatches(img1, kp1, img2, kp2, matches, None, flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "# results\n",
    "cv2.imshow('BFMater + SIFT', res)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches : 18/127, min : 24.00, max : 78.00, thresh: 34.80\n"
     ]
    }
   ],
   "source": [
    "### 8.4.4 좋은 매칭점 찾기\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img1 = cv2.imread('img/taekwonv1.jpg')\n",
    "img2 = cv2.imread('img/figures.jpg')\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# ORB 디스크립터 추출기 생성\n",
    "detector = cv2.ORB_create()\n",
    "\n",
    "# 각 영상에 대해 키포인트와 디스크립터 추출\n",
    "kp1, desc1 = detector.detectAndCompute(gray1, None)\n",
    "kp2, desc2 = detector.detectAndCompute(gray2, None)\n",
    "\n",
    "# BF-Hamming으로 매칭\n",
    "matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = matcher.match(desc1, desc2)\n",
    "\n",
    "# 매칭 결과 거리기준 오름차순 정렬\n",
    "matches = sorted(matches, key=lambda x:x.distance)\n",
    "# 최소 거리값과 최대 거리값 확보\n",
    "min_dist, max_dist = matches[0].distance, matches[-1].distance\n",
    "# 최소 거리의 20% 지점을 임계점으로 설정\n",
    "ratio = 0.2\n",
    "good_thresh = (max_dist - min_dist)*ratio + min_dist\n",
    "\n",
    "# 임계점보다 작은 매칭점만 좋은 매칭점으로 분류\n",
    "good_matches = [m for m in matches if m.distance < good_thresh]\n",
    "print(f'matches : {len(good_matches)}/{len(matches)}, min : {min_dist:.2f}, max : {max_dist:.2f}, thresh: {good_thresh:.2f}')\n",
    "\n",
    "# 좋은 매칭 결과 그리기\n",
    "res = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "# results\n",
    "cv2.imshow('Good Matches', res)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good matches:18/399\n",
      "accuracy: 9/18(0.50%)\n",
      "good matches:21/399\n",
      "accuracy: 13/21(0.62%)\n",
      "good matches:23/399\n",
      "accuracy: 14/23(0.61%)\n",
      "good matches:17/399\n",
      "accuracy: 12/17(0.71%)\n",
      "good matches:17/399\n",
      "accuracy: 9/17(0.53%)\n",
      "good matches:12/399\n",
      "accuracy: 8/12(0.67%)\n",
      "good matches:19/399\n",
      "accuracy: 10/19(0.53%)\n",
      "good matches:22/399\n",
      "accuracy: 13/22(0.59%)\n",
      "good matches:15/399\n",
      "accuracy: 10/15(0.67%)\n",
      "good matches:17/399\n",
      "accuracy: 9/17(0.53%)\n",
      "good matches:16/399\n",
      "accuracy: 10/16(0.62%)\n",
      "good matches:25/399\n",
      "accuracy: 14/25(0.56%)\n",
      "good matches:20/399\n",
      "accuracy: 10/20(0.50%)\n",
      "good matches:28/399\n",
      "accuracy: 12/28(0.43%)\n",
      "good matches:24/399\n",
      "accuracy: 13/24(0.54%)\n",
      "good matches:19/399\n",
      "accuracy: 11/19(0.58%)\n",
      "good matches:14/399\n",
      "accuracy: 7/14(0.50%)\n",
      "good matches:15/399\n",
      "accuracy: 11/15(0.73%)\n",
      "good matches:17/399\n",
      "accuracy: 10/17(0.59%)\n",
      "good matches:20/399\n",
      "accuracy: 12/20(0.60%)\n",
      "good matches:24/399\n",
      "accuracy: 14/24(0.58%)\n",
      "good matches:21/399\n",
      "accuracy: 12/21(0.57%)\n",
      "good matches:17/399\n",
      "accuracy: 10/17(0.59%)\n",
      "good matches:25/399\n",
      "accuracy: 12/25(0.48%)\n",
      "good matches:18/399\n",
      "accuracy: 10/18(0.56%)\n",
      "good matches:12/399\n",
      "accuracy: 8/12(0.67%)\n",
      "good matches:16/399\n",
      "accuracy: 10/16(0.62%)\n",
      "good matches:15/399\n",
      "accuracy: 8/15(0.53%)\n",
      "good matches:18/399\n",
      "accuracy: 11/18(0.61%)\n",
      "good matches:21/399\n",
      "accuracy: 11/21(0.52%)\n",
      "good matches:15/399\n",
      "accuracy: 11/15(0.73%)\n",
      "good matches:14/399\n",
      "accuracy: 7/14(0.50%)\n",
      "good matches:15/399\n",
      "accuracy: 9/15(0.60%)\n",
      "good matches:20/399\n",
      "accuracy: 13/20(0.65%)\n",
      "good matches:17/399\n",
      "accuracy: 10/17(0.59%)\n",
      "good matches:22/399\n",
      "accuracy: 10/22(0.45%)\n",
      "good matches:22/399\n",
      "accuracy: 16/22(0.73%)\n",
      "good matches:25/399\n",
      "accuracy: 17/25(0.68%)\n",
      "good matches:24/399\n",
      "accuracy: 17/24(0.71%)\n",
      "good matches:26/399\n",
      "accuracy: 19/26(0.73%)\n",
      "good matches:33/399\n",
      "accuracy: 19/33(0.58%)\n",
      "good matches:34/399\n",
      "accuracy: 22/34(0.65%)\n",
      "good matches:38/399\n",
      "accuracy: 28/38(0.74%)\n",
      "good matches:39/399\n",
      "accuracy: 31/39(0.79%)\n",
      "good matches:31/399\n",
      "accuracy: 19/31(0.61%)\n",
      "good matches:35/399\n",
      "accuracy: 20/35(0.57%)\n",
      "good matches:46/399\n",
      "accuracy: 28/46(0.61%)\n",
      "good matches:50/399\n",
      "accuracy: 31/50(0.62%)\n",
      "good matches:49/399\n",
      "accuracy: 27/49(0.55%)\n",
      "good matches:52/399\n",
      "accuracy: 29/52(0.56%)\n",
      "good matches:50/399\n",
      "accuracy: 31/50(0.62%)\n",
      "good matches:43/399\n",
      "accuracy: 28/43(0.65%)\n",
      "good matches:40/399\n",
      "accuracy: 25/40(0.62%)\n",
      "good matches:46/399\n",
      "accuracy: 29/46(0.63%)\n",
      "good matches:40/399\n",
      "accuracy: 25/40(0.62%)\n",
      "good matches:45/399\n",
      "accuracy: 28/45(0.62%)\n",
      "good matches:48/399\n",
      "accuracy: 27/48(0.56%)\n",
      "good matches:51/399\n",
      "accuracy: 32/51(0.63%)\n",
      "good matches:30/399\n",
      "accuracy: 17/30(0.57%)\n",
      "good matches:43/399\n",
      "accuracy: 23/43(0.53%)\n",
      "good matches:43/399\n",
      "accuracy: 22/43(0.51%)\n",
      "good matches:43/399\n",
      "accuracy: 26/43(0.60%)\n",
      "good matches:37/399\n",
      "accuracy: 20/37(0.54%)\n",
      "good matches:43/399\n",
      "accuracy: 24/43(0.56%)\n",
      "good matches:37/399\n",
      "accuracy: 18/37(0.49%)\n",
      "good matches:38/399\n",
      "accuracy: 21/38(0.55%)\n",
      "good matches:45/399\n",
      "accuracy: 22/45(0.49%)\n",
      "good matches:46/399\n",
      "accuracy: 29/46(0.63%)\n",
      "good matches:40/399\n",
      "accuracy: 25/40(0.62%)\n",
      "good matches:40/399\n",
      "accuracy: 21/40(0.53%)\n",
      "good matches:36/399\n",
      "accuracy: 20/36(0.56%)\n",
      "good matches:51/399\n",
      "accuracy: 27/51(0.53%)\n",
      "good matches:55/399\n",
      "accuracy: 32/55(0.58%)\n",
      "good matches:44/399\n",
      "accuracy: 30/44(0.68%)\n",
      "good matches:40/399\n",
      "accuracy: 22/40(0.55%)\n",
      "good matches:34/399\n",
      "accuracy: 22/34(0.65%)\n",
      "good matches:47/399\n",
      "accuracy: 34/47(0.72%)\n",
      "good matches:46/399\n",
      "accuracy: 32/46(0.70%)\n",
      "good matches:36/399\n",
      "accuracy: 22/36(0.61%)\n",
      "good matches:31/399\n",
      "accuracy: 24/31(0.77%)\n",
      "good matches:25/399\n",
      "accuracy: 16/25(0.64%)\n",
      "good matches:42/399\n",
      "accuracy: 25/42(0.60%)\n",
      "good matches:30/399\n",
      "accuracy: 20/30(0.67%)\n",
      "good matches:25/399\n",
      "accuracy: 11/25(0.44%)\n",
      "good matches:31/399\n",
      "accuracy: 19/31(0.61%)\n",
      "good matches:32/399\n",
      "accuracy: 20/32(0.62%)\n",
      "good matches:20/399\n",
      "accuracy: 11/20(0.55%)\n",
      "good matches:23/399\n",
      "accuracy: 17/23(0.74%)\n",
      "good matches:18/399\n",
      "accuracy: 14/18(0.78%)\n",
      "good matches:30/399\n",
      "accuracy: 17/30(0.57%)\n",
      "good matches:24/399\n",
      "accuracy: 15/24(0.62%)\n",
      "good matches:25/399\n",
      "accuracy: 18/25(0.72%)\n",
      "good matches:28/399\n",
      "accuracy: 20/28(0.71%)\n",
      "good matches:19/399\n",
      "accuracy: 13/19(0.68%)\n",
      "good matches:21/399\n",
      "accuracy: 15/21(0.71%)\n",
      "good matches:26/399\n",
      "accuracy: 13/26(0.50%)\n",
      "good matches:22/399\n",
      "accuracy: 13/22(0.59%)\n",
      "good matches:28/399\n",
      "accuracy: 17/28(0.61%)\n",
      "good matches:23/399\n",
      "accuracy: 17/23(0.74%)\n",
      "good matches:24/399\n",
      "accuracy: 11/24(0.46%)\n",
      "good matches:24/399\n",
      "accuracy: 15/24(0.62%)\n",
      "good matches:27/399\n",
      "accuracy: 15/27(0.56%)\n",
      "good matches:27/399\n",
      "accuracy: 14/27(0.52%)\n",
      "good matches:23/399\n",
      "accuracy: 15/23(0.65%)\n",
      "good matches:26/399\n",
      "accuracy: 17/26(0.65%)\n",
      "good matches:30/399\n",
      "accuracy: 17/30(0.57%)\n",
      "good matches:18/399\n",
      "accuracy: 12/18(0.67%)\n",
      "good matches:21/399\n",
      "accuracy: 13/21(0.62%)\n",
      "good matches:27/399\n",
      "accuracy: 14/27(0.52%)\n",
      "good matches:26/399\n",
      "accuracy: 14/26(0.54%)\n",
      "good matches:24/399\n",
      "accuracy: 15/24(0.62%)\n",
      "good matches:23/399\n",
      "accuracy: 12/23(0.52%)\n",
      "good matches:26/399\n",
      "accuracy: 13/26(0.50%)\n",
      "good matches:25/399\n",
      "accuracy: 15/25(0.60%)\n",
      "good matches:19/399\n",
      "accuracy: 11/19(0.58%)\n",
      "good matches:26/399\n",
      "accuracy: 12/26(0.46%)\n",
      "good matches:22/399\n",
      "accuracy: 13/22(0.59%)\n",
      "good matches:29/399\n",
      "accuracy: 16/29(0.55%)\n",
      "good matches:26/399\n",
      "accuracy: 14/26(0.54%)\n",
      "good matches:22/399\n",
      "accuracy: 17/22(0.77%)\n",
      "good matches:25/399\n",
      "accuracy: 15/25(0.60%)\n",
      "good matches:27/399\n",
      "accuracy: 16/27(0.59%)\n",
      "good matches:16/399\n",
      "accuracy: 11/16(0.69%)\n",
      "good matches:22/399\n",
      "accuracy: 13/22(0.59%)\n",
      "good matches:20/399\n",
      "accuracy: 15/20(0.75%)\n",
      "good matches:20/399\n",
      "accuracy: 14/20(0.70%)\n",
      "good matches:22/399\n",
      "accuracy: 15/22(0.68%)\n",
      "good matches:27/399\n",
      "accuracy: 20/27(0.74%)\n",
      "good matches:25/399\n",
      "accuracy: 15/25(0.60%)\n",
      "good matches:19/399\n",
      "accuracy: 12/19(0.63%)\n",
      "good matches:27/399\n",
      "accuracy: 20/27(0.74%)\n",
      "good matches:18/399\n",
      "accuracy: 10/18(0.56%)\n",
      "good matches:28/399\n",
      "accuracy: 14/28(0.50%)\n",
      "good matches:28/399\n",
      "accuracy: 16/28(0.57%)\n",
      "good matches:25/399\n",
      "accuracy: 15/25(0.60%)\n",
      "good matches:20/399\n",
      "accuracy: 9/20(0.45%)\n",
      "good matches:26/399\n",
      "accuracy: 15/26(0.58%)\n",
      "good matches:21/399\n",
      "accuracy: 12/21(0.57%)\n",
      "good matches:28/399\n",
      "accuracy: 15/28(0.54%)\n",
      "good matches:26/399\n",
      "accuracy: 18/26(0.69%)\n",
      "good matches:26/399\n",
      "accuracy: 14/26(0.54%)\n",
      "good matches:22/399\n",
      "accuracy: 12/22(0.55%)\n",
      "good matches:25/399\n",
      "accuracy: 16/25(0.64%)\n",
      "good matches:24/399\n",
      "accuracy: 19/24(0.79%)\n",
      "good matches:28/399\n",
      "accuracy: 18/28(0.64%)\n",
      "good matches:25/399\n",
      "accuracy: 15/25(0.60%)\n",
      "good matches:27/399\n",
      "accuracy: 14/27(0.52%)\n",
      "good matches:23/399\n",
      "accuracy: 13/23(0.57%)\n",
      "good matches:32/399\n",
      "accuracy: 17/32(0.53%)\n",
      "good matches:26/399\n",
      "accuracy: 15/26(0.58%)\n",
      "good matches:25/399\n",
      "accuracy: 17/25(0.68%)\n",
      "good matches:27/399\n",
      "accuracy: 18/27(0.67%)\n",
      "good matches:22/399\n",
      "accuracy: 11/22(0.50%)\n",
      "good matches:28/399\n",
      "accuracy: 14/28(0.50%)\n",
      "good matches:23/399\n",
      "accuracy: 14/23(0.61%)\n",
      "good matches:19/399\n",
      "accuracy: 12/19(0.63%)\n",
      "good matches:28/399\n",
      "accuracy: 16/28(0.57%)\n",
      "good matches:29/399\n",
      "accuracy: 19/29(0.66%)\n",
      "good matches:32/399\n",
      "accuracy: 13/32(0.41%)\n",
      "good matches:21/399\n",
      "accuracy: 11/21(0.52%)\n",
      "good matches:17/399\n",
      "accuracy: 9/17(0.53%)\n",
      "good matches:25/399\n",
      "accuracy: 19/25(0.76%)\n",
      "good matches:18/399\n",
      "accuracy: 9/18(0.50%)\n",
      "good matches:17/399\n",
      "accuracy: 11/17(0.65%)\n",
      "good matches:23/399\n",
      "accuracy: 12/23(0.52%)\n",
      "good matches:24/399\n",
      "accuracy: 15/24(0.62%)\n",
      "good matches:29/399\n",
      "accuracy: 20/29(0.69%)\n",
      "good matches:23/399\n",
      "accuracy: 16/23(0.70%)\n"
     ]
    }
   ],
   "source": [
    "import cv2, numpy as np\n",
    "\n",
    "img1 = None\n",
    "win_name = 'Camera Matching'\n",
    "MIN_MATCH = 10\n",
    "# ORB 검출기 생성  ---①\n",
    "detector = cv2.ORB_create(1000)\n",
    "# Flann 추출기 생성 ---②\n",
    "FLANN_INDEX_LSH = 6\n",
    "index_params= dict(algorithm = FLANN_INDEX_LSH,\n",
    "                   table_number = 6,\n",
    "                   key_size = 12,\n",
    "                   multi_probe_level = 1)\n",
    "search_params=dict(checks=32)\n",
    "matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "# 카메라 캡쳐 연결 및 프레임 크기 축소 ---③\n",
    "cap = cv2.VideoCapture(0)              \n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "while cap.isOpened():       \n",
    "    ret, frame = cap.read() \n",
    "    if img1 is None:  # 등록된 이미지 없음, 카메라 바이패스\n",
    "        res = frame\n",
    "    else:             # 등록된 이미지 있는 경우, 매칭 시작\n",
    "        img2 = frame\n",
    "        gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "        gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "        # 키포인트와 디스크립터 추출\n",
    "        kp1, desc1 = detector.detectAndCompute(gray1, None)\n",
    "        kp2, desc2 = detector.detectAndCompute(gray2, None)\n",
    "        # k=2로 knnMatch\n",
    "        matches = matcher.knnMatch(desc1, desc2, 2)\n",
    "        # 이웃 거리의 75%로 좋은 매칭점 추출---②\n",
    "        ratio = 0.75\n",
    "        good_matches = [m[0] for m in matches \\\n",
    "                if len(m) == 2 and m[0].distance < m[1].distance * ratio]\n",
    "        print('good matches:%d/%d' %(len(good_matches),len(matches)))\n",
    "        # 모든 매칭점 그리지 못하게 마스크를 0으로 채움\n",
    "        matchesMask = np.zeros(len(good_matches)).tolist()\n",
    "        # 좋은 매칭점 최소 갯수 이상 인 경우\n",
    "        if len(good_matches) > MIN_MATCH: \n",
    "            # 좋은 매칭점으로 원본과 대상 영상의 좌표 구하기 ---③\n",
    "            src_pts = np.float32([ kp1[m.queryIdx].pt for m in good_matches ])\n",
    "            dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good_matches ])\n",
    "            # 원근 변환 행렬 구하기 ---⑤\n",
    "            mtrx, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "            accuracy=float(mask.sum()) / mask.size\n",
    "            print(\"accuracy: %d/%d(%.2f%%)\"% (mask.sum(), mask.size, accuracy))\n",
    "            if mask.sum() > MIN_MATCH:  # 정상치 매칭점 최소 갯수 이상 인 경우\n",
    "                # 이상점 매칭점만 그리게 마스크 설정\n",
    "                matchesMask = mask.ravel().tolist()\n",
    "                # 원본 영상 좌표로 원근 변환 후 영역 표시  ---⑦\n",
    "                h,w, = img1.shape[:2]\n",
    "                pts = np.float32([ [[0,0]],[[0,h-1]],[[w-1,h-1]],[[w-1,0]] ])\n",
    "                dst = cv2.perspectiveTransform(pts,mtrx)\n",
    "                img2 = cv2.polylines(img2,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
    "        # 마스크로 매칭점 그리기 ---⑨\n",
    "        res = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, \\\n",
    "                            matchesMask=matchesMask,\n",
    "                            flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "    # 결과 출력\n",
    "    cv2.imshow(win_name, res)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:    # Esc, 종료\n",
    "            break          \n",
    "    elif key == ord(' '): # 스페이스바를 누르면 ROI로 img1 설정\n",
    "        x,y,w,h = cv2.selectROI(win_name, frame, False)\n",
    "        if w and h:\n",
    "            img1 = frame[y:y+h, x:x+w]\n",
    "else:\n",
    "    print(\"can't open camera.\")\n",
    "cap.release()                          \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8.5 객체 추적\n",
    "### 8.5.1 동영상 배경 제거\n",
    "### MOG2 배경 제거\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture('img/walking.avi')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "delay = int(1000/fps)\n",
    "\n",
    "# 배경 제거 객체 생성\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # 배경 제거 마스크 계산\n",
    "    fgmask = fgbg.apply(frame)\n",
    "    cv2.imshow('frame', frame)\n",
    "    cv2.imshow('bgsub', fgmask)\n",
    "    if cv2.waitKey(delay) & 0xff == 27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### p.358 희소 옵티컬 플로\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture('img/walking.avi')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) # 프레임 수 구하기\n",
    "delay = int(1000/fps)\n",
    "# 추적 경로를 그리기 위한 랜덤 색상\n",
    "color = np.random.randint(0,255,(200,3))\n",
    "lines = None  #추적 선을 그릴 이미지 저장 변수\n",
    "prevImg = None  # 이전 프레임 저장 변수\n",
    "# calcOpticalFlowPyrLK 중지 요건 설정\n",
    "termcriteria =  (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret,frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    img_draw = frame.copy()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # 최초 프레임 경우\n",
    "    if prevImg is None:\n",
    "        prevImg = gray\n",
    "        # 추적선 그릴 이미지를 프레임 크기에 맞게 생성\n",
    "        lines = np.zeros_like(frame)\n",
    "        # 추적 시작을 위한 코너 검출  ---①\n",
    "        prevPt = cv2.goodFeaturesToTrack(prevImg, 200, 0.01, 10)\n",
    "    else:\n",
    "        nextImg = gray\n",
    "        # 옵티컬 플로우로 다음 프레임의 코너점  찾기 ---②\n",
    "        nextPt, status, err = cv2.calcOpticalFlowPyrLK(prevImg, nextImg, \\\n",
    "                                        prevPt, None, criteria=termcriteria)\n",
    "        # 대응점이 있는 코너, 움직인 코너 선별 ---③\n",
    "        prevMv = prevPt[status==1]\n",
    "        nextMv = nextPt[status==1]\n",
    "        for i,(p, n) in enumerate(zip(prevMv, nextMv)):\n",
    "            px,py = p.ravel()\n",
    "            nx,ny = n.ravel()\n",
    "            # 이전 코너와 새로운 코너에 선그리기 ---④\n",
    "            cv2.line(lines, (int(px), int(py)), (int(nx), int(ny)), color[i].tolist(), 2)\n",
    "            # 새로운 코너에 점 그리기\n",
    "            cv2.circle(img_draw, (int(nx), int(ny)), 2, color[i].tolist(), -1)\n",
    "        # 누적된 추적 선을 출력 이미지에 합성 ---⑤\n",
    "        img_draw = cv2.add(img_draw, lines)\n",
    "        # 다음 프레임을 위한 프레임과 코너점 이월\n",
    "        prevImg = nextImg\n",
    "        prevPt = nextMv.reshape(-1,1,2)\n",
    "\n",
    "    cv2.imshow('OpticalFlow-LK', img_draw)\n",
    "    key = cv2.waitKey(delay)\n",
    "    if key == 27 : # Esc:종료\n",
    "        break\n",
    "    elif key == 8: # Backspace:추적 이력 지우기\n",
    "        prevImg = None\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CamShift 객체 추적\n",
    "### p.358 희소 옵티컬 플로\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "roi_hist = None # 추적 객체 히스토그램 저장 변수\n",
    "win_name = 'Camshift Tracking'\n",
    "termination =  (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret,frame = cap.read()\n",
    "    img_draw = frame.copy()\n",
    "    \n",
    "    if roi_hist is not None:\n",
    "        # 전체 영상 hsv 컬러로 변환\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        # 전체 영상 히스토그램과 roi 히스토그램 역투영\n",
    "        dst = cv2.calcBackProject([hsv], [0], roi_hist, [0, 180], 1)\n",
    "        # 역투영 결과와 초기 추적 위치로 평균 이동 추적\n",
    "        ret, (x, y, w, h) = cv2.CamShift(dst, (x, y, w, h), termination)\n",
    "        # 새로운 위치에 사각형 표시\n",
    "        cv2.rectangle(img_draw, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        # 컬러 영상과 역투영 영상을 통합해 출력\n",
    "        result = np.hstack((img_draw, cv2.cvtColor(dst, cv2.COLOR_GRAY2BGR)))\n",
    "    \n",
    "    else:\n",
    "        cv2.putText(img_draw, 'Hit the spacce to set target to track', (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SCRIPT_SIMPLEX, 1, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "        result = img_draw\n",
    "\n",
    "    cv2.imshow(win_name, result)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == 27 : # Esc:종료\n",
    "        break\n",
    "    elif key == ord(' '): # 스페이스바, ROI 설정\n",
    "        x, y, w, h = cv2.selectROI(win_name, frame, False)\n",
    "        if w and h:\n",
    "            roi = frame[y:y+h, x:x+w]\n",
    "            roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "            mask = None\n",
    "            roi_hist = cv2.calcHist([roi], [0], mask, [180], [0, 180])\n",
    "            cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)\n",
    "    else:\n",
    "        roi_hist = None\n",
    "else:\n",
    "    print('No camera!')\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5d44725d55c3c25373c9cea7b363fe9d5696c3b23e8383192da0211bfba569d6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
