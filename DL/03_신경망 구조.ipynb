{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "03_신경망 구조.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al1iR8arJ9Aa"
      },
      "source": [
        "# DNN (Deep Neural Network)\n",
        "\n",
        "## 신경망 구성요소\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYm6fWNoJ9Aj"
      },
      "source": [
        "### Train(학습) 프로세스\n",
        "![image-2.png](attachment:image-2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyqCkwX7J9Ak"
      },
      "source": [
        "- <span style='font-size:1.1em;font-weight:bold'>층(Layer)</span>: Network를 구성하는 Layer(층)\n",
        "- <span style='font-size:1.1em;font-weight:bold'>손실함수(loss function)</span>: 가중치를 어떻게 업데이트할 지 예측결과와 Ground truth(실제정답) 사이의 차이를 정의\n",
        "- <span style='font-size:1.1em;font-weight:bold'>optimizer</span>: 가중치를 업데이트하여 모델의 성능을 최적화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c6PuvpZJ9Am"
      },
      "source": [
        "## 유닛/노드/뉴런 (Unit, Node, Neuron)\n",
        "- Tensor를 입력받아 처리후 tensor를 출력하는 데이터 처리 모듈 \n",
        "    - Input -> Output\n",
        "- 입력 값에 Weight(가중치)를 곱하고 bias(편향)을 더한 결과를 Activation 함수에 넣어 최종 결과를 출력하는 계산을 처리한다\n",
        "- \n",
        "    \n",
        "    ![image.png](attachment:image.png)\n",
        "    - **Input vector(입력값)**: $\\large\\mathbb x=(x_1, x_2, x_3)^T$\n",
        "    - **Weights(가중치)**: $\\large\\mathbb w = (w_1, w_2, w_3)^T$\n",
        "    - **Bias(편향)**: $\\large b \\in \\mathbb R$\n",
        "    - **Activation function(활성함수)**: $\\large\\sigma(\\cdot)$\n",
        "        - 선형결합한 결과를 비선형화 시키는 목적\n",
        "        - 다양한 비선형 함수들을 사용한다.\n",
        "\n",
        "       <center> $\\LARGE z=w_1 x_1 + w_2 x_2 + w_3 x_3 + b \\Leftrightarrow z = \\mathbb w^T \\mathbb x + b$ <br><br>\n",
        "       $\\LARGE a=\\sigma( z)$  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3mInnXnJ9An"
      },
      "source": [
        "## 레이어/층(Layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPZcbVJEJ9Ao"
      },
      "source": [
        "- **Input Layer(입력층)**\n",
        "    - 입력값들을 받아 Hidden Layer에 전달하는 노드들로 구성된 Layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKyBKbYOJ9Ap"
      },
      "source": [
        "- **Output Layer(출력층)**\n",
        "    - 예측결과를 출력하는 노드들로 구성된 Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX3DV5hIJ9Ar"
      },
      "source": [
        "- **Hidden Layer(은닉층)**\n",
        "    - Input Layer와 Output Layer사이에 존재하는 Layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vrqd2xKeJ9As"
      },
      "source": [
        "- 대부분 Layer들은 학습을 통해 최적화할 Paramter를 가짐 \n",
        "    - Dropout, Pooling Layer와 같이 Parameter가 없는 layer도 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3X7Z_UbJ9At"
      },
      "source": [
        "- Layer들의 연결한 것을 **<font size=5>Network</font>** 라고 한다.\n",
        "    - 딥러닝은 Layer들을 깊게 쌓은 것을 말한다.(여러 Layer들을 연결한 것)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KP9OldMJ9Au"
      },
      "source": [
        "- 목적, 구현 방식에 따라 다양한 종류의 Layer들이 있다.\n",
        "    - **Fully Connected Layer (Dense layer)**\n",
        "        - 추론 단계에서 주로 사용\n",
        "    - **Convolution Layer**\n",
        "        - 이미지 Feature extraction으로 주로 사용\n",
        "    - **Recurrent Layer** \n",
        "        - Sequential(순차) 데이터의 Feature extraction으로 주로 사용\n",
        "    - **Embedding Layer**\n",
        "        - Text 데이터의 Feature extraction으로 주로 사용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2es6t5RJ9Au"
      },
      "source": [
        "- **API** \n",
        "    - https://keras.io/api/layers/\n",
        "    - https://www.tensorflow.org/api_docs/python/tf/keras/layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqavOtdZJ9Aw"
      },
      "source": [
        "## 모델 (Network)\n",
        "- Layer를 연결한 것이 Deep learning 모델다.\n",
        "- 이전 레이어의 출력을 input으로 받아 처리 후 output으로 출력하는 레이어들을 연결한다.\n",
        "- 적절한 network 구조(architecture)를 찾는 것은 공학적이기 보다는 경험적(Art)접근이 필요하다.\n",
        "\n",
        "![image-2.png](attachment:image-2.png)\n",
        "- **API**\n",
        "    - https://keras.io/api/models/\n",
        "    - https://www.tensorflow.org/api_docs/python/tf/keras/models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmWXHvauJ9Ay"
      },
      "source": [
        "## 활성 함수 (Activation Function)\n",
        "- 각 유닛이 입력과 Weight간에 가중합을 구한 뒤 출력결과를 만들기 위해 거치는 함수\n",
        "- 같은 층(layer)의 모든 유닛들은 같은 활성 함수를 가진다.\n",
        "- **출력 레이어**의 경우 출력하려는 문제에 맞춰 나오도록 하는 함수를 사용한다.\n",
        "- **은닉층 (Hidden Layer)**의 경우 비선형성을 주는 것을 목적으로 한다.\n",
        "    - **ReLU** 함수를 주로 사용한다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pcc3xFOJ9Ay"
      },
      "source": [
        "### 주요 활성함수(Activation Function)\n",
        "\n",
        "- ### Sigmoid (logistic function)\n",
        "    ![image-2.png](attachment:image-2.png)\n",
        "    \n",
        "    - $$\\large \\sigma(z) = \\frac 1 {1+e^{-z}}$$\n",
        "    - 출력값의 범위\n",
        "        - $0<sigmoid(z)<1$\n",
        "    - 한계\n",
        "        - 초기 딥러닝의 hidden layer(은닉층)의 activation function(활성함수)로 많이 사용 되었다.\n",
        "        - 층을 깊게 쌓을 경우 **기울기 소실(Gradient Vanishing)** 문제를 발생시켜 학습이 안되는 문제가 있다.\n",
        "        - 함수값의 중심이 0이 아니어서 학습이 느려지는 단점이 있다.\n",
        "            - X의 값이 0일때 0.5를 반환한다.\n",
        "    - **Binary classification(이진 분류)를 위한 네트워크의 Output layer(출력층)의 활성함수로 사용된다.**\n",
        "        - 위와 같은 한계때문에 hidden layer(은닉층)의 activation function(활성함수)로는 잘 사용되지 않는다.\n",
        "\n",
        "> **<font size=5>기울기 소실(Gradient Vanishing) 문제란</font>**\n",
        "> - 최적화 과정에서 gradient가 0과 밑단층 (Bottom Layer)의 가중치들이 학습이 안되는 현상"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZDv6fERJ9A0"
      },
      "source": [
        "- ### Hyperbolic tangent\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "    - $$\\large  tanh(z) = \\cfrac{e^{z} - e^{-z}}{{e^{z} + e^{-z}}}$$\n",
        "    - 출력값의 범위\n",
        "        - $-1<tanh(z)<1$\n",
        "    - Output이 0을 중심으로 분포하므로 sigmoid보다 학습에 효율 적이다.\n",
        "    - 기울기 소실(Gradient Vanishing) 문제를 발생시킨다.\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77zL9fx8J9A1"
      },
      "source": [
        "- ### ReLU(Rectified Linear Unit)\n",
        "![image-2.png](attachment:image-2.png)\n",
        "    $$\\large  ReLU(z)=max(0,z)$$\n",
        "    - 기울기 소실(Gradient Vanishing) 문제를 어느정도 해결\n",
        "    - 0이하의 값(z <= 0)들에 대해 뉴런이 죽는 단점이 있다. (Dying ReLU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibI5dBcjJ9A2"
      },
      "source": [
        "- ### Leaky ReLU\n",
        "    ![image-4.png](attachment:image-4.png)\n",
        "    <br><br>\n",
        "     $$\\large  LeakyReLU(z)=max(\\alpha z,z)\\\\\n",
        "     0< \\alpha <1$$\n",
        "    <br><br>\n",
        "    - ReLU의 Dying ReLU 현상을 해결하기 위해 나온 함수\n",
        "    - 음수 z를 0으로 반환하지 않고 alpah (0 ~ 1 사이 실수)를 곱해 반환한다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqfyUOHTJ9A2"
      },
      "source": [
        "- ### Softmax\n",
        "    $$\\large  Softmax(z_j) = \\frac{exp(z_j)}{\\sum_{k=1}^K exp(z_k)}\\\\ \\:j=1,2, \\ldots, K$$\n",
        "    - **Multi-class classification(다중 분류)를 위한 네트워크의 Output layer(출력층)의 활성함수로 주로 사용된다.** \n",
        "        - 은닉층의 활성함수로 사용하지 않는다.\n",
        "    - Layer의 unit들의 출력 값들을 정규화 하여 각 class에 대한 확률값으로 변환한다.\n",
        "        - 출력노드들의 값은 0 ~ 1사이의 실수로 변환되고 그 값들의 총합은 1이 된다.\n",
        "    \n",
        "\n",
        "\n",
        "- **API**\n",
        "    - https://keras.io/api/layers/activations/\n",
        "    - https://www.tensorflow.org/api_docs/python/tf/keras/activations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVXVY4J4J9A5"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqROBhz2J9A6"
      },
      "source": [
        "## 손실함수(Loss function, 비용함수)\n",
        "- Model이 출력한 예측값(prediction) $\\hat y^{(i)}$와 실제 데이터(output) $y^{(i)}$의 차이를 계산하는 함수\n",
        "- 네트워크 모델을 훈련하는 동안 Loss 함수가 계산한 Loss값(손실)이 최소화 되도록 파라미터(가중치와 편향)을 업데이트한다.\n",
        "    - 즉 Loss함수는 최적화 시작이 되는 값이다.\n",
        "- 네트워크 모델이 해결하려는 문제의 종류에 따라 다른 Loss함수를 사용한다.\n",
        "\n",
        "- <font size=\"5\" color=\"red\">해결하려는 문제의 종류에 따라 표준적인 Loss function이 있다.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnZsnbi0J9A6"
      },
      "source": [
        "- ### Binary classification (이진 분류)\n",
        "    - 두 개의 클래스를 분류\n",
        "        - 둘 중 하나\n",
        "    - 예) 문장을 입력하여 긍정/부정 구분\n",
        "    - **binary_crossentropy**를 loss function으로 사용\n",
        "    \n",
        "    - $$\\large Loss(\\hat y^{(i)} ,y^{(i)}) = - y^{(i)} log(\\hat y^{(i)} ) - (1- y^{(i)}) log (1-\\hat y^{(i)} )$$\n",
        "    \n",
        "    $y^{(i)}$: 실제 값(Ground Truth), $\\hat y^{(i)}$: 예측확률"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLdPz4N8J9A7"
      },
      "source": [
        "- ### Multi-class classification (다중 클래스 분류)\n",
        "    - 두 개 이상의 클래스를 분류 \n",
        "        - 여러개 중 하나\n",
        "    - 예) 이미지를 0,1,2,...,9로 구분\n",
        "    - **categorical_crossentropy**를 loss function으로 사용 \n",
        "    - $$\\large Loss(\\hat y^{(i)} ,y^{(i)}) = - \\sum_{c=1}^C y_c^{(i)} log(\\hat y_c^{(i)} ) $$\n",
        "\n",
        "    $y^{(i)}$: 실제 값(Ground Truth), $\\hat y_c^{(i)}$: class별 예측확률"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKu6GanEJ9A8"
      },
      "source": [
        "- ### Regression (회귀)\n",
        "    - 연속형 값을 에측 \n",
        "    - 예) 주가 예측 \n",
        "    - **Mean squared error**를 loss function으로 사용 \n",
        "        - \"mse\" 로 지정\n",
        "    - $$\\large Loss(\\hat y^{(i)} ,y^{(i)}) = \\frac  1 2 (\\hat y^{(i)} - y^{(i)})^2$$\n",
        "    \n",
        "    $y^{(i)}$: 실제 값(Ground Truth), $\\hat y^{(i)}$: 예측 값"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucK8_43sJ9A9"
      },
      "source": [
        "- **API**\n",
        "    - https://keras.io/api/losses/\n",
        "    - https://www.tensorflow.org/api_docs/python/tf/keras/losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzbRwEI2J9A9"
      },
      "source": [
        "## 평가지표 (Metrics)\n",
        "- 모델의 성능을 평가하는 지표\n",
        "- 손실함수(Loss Function)와 차이\n",
        "    - 손실함수는 모델을 학습할 때 가중치 업데이트를 위한 오차를 구할 때 사용한다.\n",
        "    - 평가지표 함수는 모델의 성능이 확인하는데 사용한다. \n",
        "\n",
        "- **API**\n",
        "    - https://keras.io/api/metrics/\n",
        "    - https://www.tensorflow.org/api_docs/python/tf/keras/metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-01T07:06:19.300768Z",
          "start_time": "2021-11-01T07:06:19.282735Z"
        },
        "id": "byLJK7SeJ9A-"
      },
      "source": [
        "## 문제별 출력레이어 Activation 함수, Loss 함수\n",
        "\n",
        "문제형태|Activation함수|Loss 함수\n",
        " :- | :- | :-\n",
        "이진분류(Binary Classification)|sigmoid| binary_crossentropy\n",
        "다중분류(Multi-class Classification)|softmax| categorical_crossentropy<br>sparse_categorical_crossentropy\n",
        "회귀(Regression)|None|MSE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wROELMV1J9A-"
      },
      "source": [
        "# Optimizer (최적화 방법)\n",
        "\n",
        "- Training시 모델 네트워크의 parameter를 데이터에 맞춰 최적화 하는 알고리즘\n",
        "    - Deep Learning은 경사하강법(Gradient Descent)와 오차 역전파(back propagation) 알고리즘을 기반으로 파라미터들을 최적화한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tujLnVhuJ9A_"
      },
      "source": [
        "## Gradient Decent (경사하강법)\n",
        "- ### 최적화 \n",
        "    - 모델 네트워크가 출력한 결과와 실제값(Ground Truth)의 차이를 정의하는 함수를 **Loss function(손실함수, 비용함수)** 라고 한다.\n",
        "    - Training 시 Loss function이 출력하는 값을 줄이기 위해 파라미터(weight, bias)를 update 과정을 **최적화(Optimization)** 이라고 한다.\n",
        "- ### Gradient Decent(경사하강법)\n",
        "    - 최적화를 위해 파라미터들에 대한 Loss function의 Gradient값을 구해 Gradient의 반대 방향으로 일정크기 만큼 파라미터들을 업데이트 하는 것을 경사하강법이라고 한다.\n",
        "        \n",
        "$$\\large W_{new} = W-\\alpha\\frac{\\partial}{\\partial {W}}Loss(W)\\\\W: 파라미터\\: \\alpha:학습률$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHq_OzFCJ9BA"
      },
      "source": [
        "## 오차 역전파(Back Propagation) \n",
        "\n",
        "- 딥러닝 학습시 파라미터를 최적화 할 때 추론한 역방향으로 loss를 전달하여 단계적으로 파라미터들을 업데이트한다.\n",
        "    - Loss에서부터(뒤에서부터) 한계단씩 미분해 gradient 값을 구하고 이를 Chain rule(연쇄법칙)에 의해 곱해가면서 파라미터를 최적화한다.\n",
        "    - 출력에서 입력방향으로 계산하여 역전파(Back propagation)라고 한다.\n",
        "        - 추론의 경우 입력에서 출력 방향으로 계산하며 이것은 순전파(Forward propagation)이라고 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMKXnXvgJ9BA"
      },
      "source": [
        "### 계산 그래프 (Computational Graph)\n",
        "- 복잡한 계산 과정을 자료구조의 하나인 그래프로 표현한 것\n",
        "- 그래프는 노드(Node)와 엣지(Edge)로 구성됨.\n",
        "    - 노드: 연산을 정의\n",
        "    - 엣지: 데이터가 흘러가는 방향"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhnG8gQ_J9BB"
      },
      "source": [
        "### 계산 그래프의 예\n",
        "- 슈퍼에서 1개에 100원인 사과를 2개 샀을 때 지불할 금액은 어떻게 될까? 단 부가세는 10% 부과된다.\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q17rqeseJ9BB"
      },
      "source": [
        "### 계산 그래프 장점\n",
        "- 계산 그래프를 사용한 문제 풀이 절차\n",
        "    - 계산 그래프를 구성\n",
        "    - 계산 방향을 결정\n",
        "        - 왼쪽에서 오른쪽 방향으로 계산: **순전파(Forward propagation)**\n",
        "        - 오른쪽 방향에서 왼쪽 방향으로 계산: **역전파(Back propagation)**\n",
        "- 특징/장점\n",
        "    - **국소적 계산**을 통해 결과를 얻는다.\n",
        "        - 각 노드의 계산은 자신과 관계된 정보(입력 값들)만 가지고 계산한 뒤 그 결과를 다음으로 출력한다.\n",
        "    - 복잡한 계산을 단계적으로 나눠 처리하므로 문제를 단순하게 만들어 계산할 수 있다.\n",
        "        - **딥러닝에서 역전파를 이용해 각 가중치 업데이트를 위한 미분(기울기) 계산을 효율적으로 할 수 있게 된다.**\n",
        "    - 중간 계산결과를 보관할 수 있다.\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMvOEg5wJ9BC"
      },
      "source": [
        "## 합성함수의 미분\n",
        "- **<font size='5'>합성함수 : 여러 함수로 구성된 함수</font>**\n",
        "$$\n",
        "\\large z = (x+y)^2\\\\\n",
        "\\large z = t^2\\\\\n",
        "\\large t=x+y\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rSM2Lk6J9BC"
      },
      "source": [
        "- **<font size='5'>연쇄 법칙(Chain Rule)</font>**\n",
        "    - 합성함수의 미분은 합성함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다.\n",
        "$$\n",
        "\\large\\cfrac{\\partial z}{\\partial x} = \\cfrac{\\partial z}{\\partial t} \\cfrac{\\partial t}{\\partial x} \\\\\n",
        "\\cfrac{\\partial z}{\\partial t} = 2t,\\;\\cfrac{\\partial t}{\\partial x} = 1 \\\\\n",
        "\\large\\cfrac{\\partial z}{\\partial x}=2t \\times 1 = 2(x+y)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaJiVN2gJ9BE"
      },
      "source": [
        "### 연쇄 법칙과 계산 그래프"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tySiqDFyJ9BE"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKEoybP5J9BF"
      },
      "source": [
        "## 딥러닝 네트워크에서 최적화 예"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_style": "center",
        "id": "sxO30qcvJ9BF"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_style": "split",
        "id": "n5zvcicCJ9BF"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_style": "split",
        "id": "2-dIPR0lJ9BG"
      },
      "source": [
        "- ### $W_{11}$을 업데이트 하기 위한 미분값은?\n",
        "$$\n",
        "\\large\\frac{\\partial}{\\partial {W_{11}}}{Loss} = ????\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_style": "split",
        "id": "2yeF5gJ6J9BH"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6voxqsWJ9BV"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylRajx3VJ9BV"
      },
      "source": [
        "- **순전파(forward propagation): 추론**\n",
        "- **역전파(back propagation): 학습시 파라미터(weight) 업데이트**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppOR_pC8J9BW"
      },
      "source": [
        "## 파라미터 업데이트 단위\n",
        "\n",
        "- **Batch Gradient Decent (배치 경사하강법)**\n",
        "    - Loss를 계산할 때 전체 학습데이터를 사용해 그 평균값을 기반으로 파라미터를 최적화한다.\n",
        "    - 많은 계산량이 필요해서 속도가 느리다. 학습 데이터가 클 경우 메모리가 부족할 수 있다.\n",
        "\n",
        "- **Mini Batch Stochastic Gradient Decent (미니배치 확률적 경사하강법)**\n",
        "    - Loss를 계산할 때 전체 데이터를 다 사용하지 않고 지정한 데이터 양(batch size) 만큼 마다 계산해 파라미터를 업데이트 한다.\n",
        "    - 계산은 빠른 장점이 있지만 최적값을 찾아 가는 방향이 불안정 하여 부정확 하다. 그러나 반복 횟수를 늘리면 Batch 방식과 유사한 결과로 수렴한다.\n",
        "\n",
        "> **스텝(Step)**:  한번 파라미터를 업데이트하는 단위\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAM5gz9sJ9BY"
      },
      "source": [
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb-FL1RsJ9BY"
      },
      "source": [
        "## SGD를 기반으로 한 주요 옵티마이저\n",
        "- 방향성을 개선한 최적화 방법\n",
        "    - Momentum\n",
        "    - NAG(Nesterov Accelerated Gradient)\n",
        "- 학습률을 개선한 최적화 방법\n",
        "    - Adagrad\n",
        "    - RMSProp\n",
        "- 방향성 + 학습률 개선 최적화 방법\n",
        "    - Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ0kYBOuJ9BZ"
      },
      "source": [
        "![image.png](attachment:image.png)\n",
        "<center>[출처] https://www.slideshare.net/yongho/ss-79607172</center>\n",
        "\n",
        "- **API**\n",
        "    - https://keras.io/api/optimizers/\n",
        "    - https://www.tensorflow.org/api_docs/python/tf/keras/activations\n",
        "    "
      ]
    }
  ]
}